{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layer Forward Pass Network with Sigmoid Activation \n",
    "\n",
    "Below, you'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.\n",
    "\n",
    "Things to do:\n",
    "\n",
    "- Calculate the input to the hidden layer.\n",
    "- Calculate the hidden layer output.\n",
    "- Calculate the input to the output layer.\n",
    "- Calculate the output of the network\n",
    "\n",
    "**Perceptron Function:** Receives Two Inputs, applying linear combination function as linear_combination = weight1 . input1 + weight2 . input2\n",
    "\n",
    "**Activation Function:** In this case, Sigmoid Function with Bias: output as range of probabilities (of success)\n",
    "\n",
    "**Error** Error is nothing but known output (y) minus computed output (y-hat)\n",
    "\n",
    "**Error Term** Error minus derivative of sigmoid\n",
    "\n",
    "**Gradient Descent Step** Gives us revised weights, or 'delta w' -- It is learning rate (eta) multipliedBy error_term  multipliedBy input(x)\n",
    "\n",
    "**Gradient Descent** After every step computed i.e. delta of weights. Updated weights with delta weights from Gradient Step. We will be follwing MSE (Mean Squared Error) instead of SSE (Sum of Squared Errors) avoid Gradient from Divergering & keep it computationally efficient .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.27630002065852294)\n",
      "('Train loss: ', 0.27487280940102665)\n",
      "('Train loss: ', 0.2734814690053808)\n",
      "('Train loss: ', 0.27212535119812675)\n",
      "('Train loss: ', 0.27080379729958337)\n",
      "('Train loss: ', 0.2695161402601928)\n",
      "('Train loss: ', 0.2682617065761968)\n",
      "('Train loss: ', 0.26703981808591765)\n",
      "('Train loss: ', 0.2658497936485804)\n",
      "('Train loss: ', 0.26469095070807397)\n",
      "Prediction accuracy: 0.425\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error gradient in output unit\n",
    "        output_error = error * output * (1 - output)\n",
    "\n",
    "        # TODO: propagate errors to hidden layer\n",
    "        hidden_error = np.dot(output_error, weights_hidden_output) * \\\n",
    "                       hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error * hidden_output\n",
    "        del_w_input_hidden += hidden_error * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
